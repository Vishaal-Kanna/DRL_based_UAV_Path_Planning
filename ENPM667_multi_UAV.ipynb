{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "\n",
    "#implementation for three UAV case with single FR\n",
    "\n",
    "\n",
    "e = 2.718\n",
    "\n",
    "Episode_length = 20\n",
    "\n",
    "#map parameters based on obstacles\n",
    "alpha = 0.7   \n",
    "beta = 10\n",
    "\n",
    "#defining the action space\n",
    "d = 5\n",
    "del_theta = 90*3.14/180\n",
    "\n",
    "#measurement noise parameters\n",
    "muLOS = 0\n",
    "stdLOS = 0.5\n",
    "muNLOS = 5\n",
    "stdNLOS = 5\n",
    "\n",
    "def gaussianpdf(x, mu, std):\n",
    "    pdfx = (math.pow(e,(-0.5*math.pow((x-mu)/std,2))))/(std*math.pow(2*3.14,0.5))\n",
    "    return pdfx  \n",
    "\n",
    "#maximum likelihood function for state estimotor(Not called in this code)\n",
    "def maxlf(uavx, uavy, uavz, tx, ty, tz):\n",
    "\n",
    "    d = math.pow(math.pow((uavx-tx),2) + math.pow((uavy-ty),2) + math.pow((uavz-tz),2),0.5)\n",
    "    elevation = math.asin((uavz-tz)/d)\n",
    "    lamda = 1/(1+alpha*math.pow(e,-beta*(elevation-alpha)))\n",
    "    \n",
    "    for i in range(tx-uavx-10,tx-uavx+10):\n",
    "        x=i\n",
    "        yij = lamda*gaussianpdf(x,tx-uavx+muLOS,stdLOS) + (1-lamda)*gaussianpdf(x,tx-uavx+muNLOS,stdNLOS)\n",
    "        plt.scatter(i,yij)\n",
    "    return yij\n",
    "\n",
    "#function to generate the next states of the target based on action 'a'\n",
    "def statetransition(stateold,a):\n",
    "    delK = 1\n",
    "    alpha0 = 0.95\n",
    "    \n",
    "    zerom = np.matrix([[0, 0, 0],[0, 0, 0], [0, 0, 0]])\n",
    "    phibar = np.matrix([[1, delK, delK*delK/2],[0, 1, delK], [0, 0, alpha0]])\n",
    "    Lbarv = np.matrix([delK*delK/2, delK, 0])\n",
    "    \n",
    "    statenew = np.dot(phibar,stateold.T).T + np.dot(a,Lbarv).T\n",
    "            \n",
    "    return statenew\n",
    "\n",
    "#Prediction of target state(Not called in this code)\n",
    "def target_pred(xvectorold, yvectorold, zvectorold, l):\n",
    "    \n",
    "    target_action = np.zeros((5,3))\n",
    "    target_action[0] = np.matrix([0, 0, 0])\n",
    "    target_action[1] = np.matrix([1, 0, 0])\n",
    "    target_action[2] = np.matrix([-1, 0, 0])\n",
    "    target_action[3] = np.matrix([0, 1, 0])\n",
    "    target_action[4] = np.matrix([0, -1, 0])\n",
    "\n",
    "    lbar = np.zeros(5)\n",
    "    lbar[0] = 0\n",
    "    lbar[1] = 1\n",
    "    lbar[2] = 2\n",
    "    lbar[3] = 3\n",
    "    lbar[4] = 4\n",
    "    \n",
    "    for j in range(0,5):\n",
    "        if lbar[j] == l:\n",
    "            transprob = 0.1\n",
    "        else:\n",
    "            transprob = 0.225\n",
    "        xvectornew = statetransition(xvectorold, target_action[int(lbar[j])][0]).T\n",
    "        yvectornew = statetransition(yvectorold, target_action[int(lbar[j])][1]).T\n",
    "        zvectornew = statetransition(zvectorold, target_action[int(lbar[j])][2]).T\n",
    "           \n",
    "    return xvectornew, yvectornew, zvectornew, state_prob\n",
    "\n",
    "class UAV(Env):\n",
    "    def __init__(self,x,y,z):\n",
    "        # Actions UAV can take\n",
    "        self.uav_action_space = Discrete(4)\n",
    "        self.x1 = x\n",
    "        self.y1 = y\n",
    "        self.z1 = z\n",
    "        \n",
    "        self.tx = 30\n",
    "        self.ty = 30\n",
    "        self.tz = 0\n",
    "        self.txdot = 0.4\n",
    "        self.tydot = 0.4\n",
    "        self.tzdot = 0\n",
    "        self.txddot = 0\n",
    "        self.tyddot = 0\n",
    "        self.tzddot = 0\n",
    "        \n",
    "        \n",
    "        #Set episode length\n",
    "        self.episode_length = Episode_length\n",
    "        \n",
    "    def step(self, x2,y2,z2,x3,y3,z3, action, lamdaprev,i):\n",
    "        # Apply UAV action\n",
    "        self.x1 += d*math.cos(action*del_theta)\n",
    "        self.y1 += d*math.sin(action*del_theta)\n",
    "        self.z1 += 0\n",
    "        \n",
    "        target_action = np.zeros((5,3))\n",
    "        target_action[0] = np.matrix([0, 0, 0])\n",
    "        target_action[1] = np.matrix([1, 0, 0])\n",
    "        target_action[2] = np.matrix([-1, 0, 0])\n",
    "        target_action[3] = np.matrix([0, 1, 0])\n",
    "        target_action[4] = np.matrix([0, -1, 0])\n",
    "        \n",
    "        if i!=-1:\n",
    "           a = target_action[random.randint(0,4)]\n",
    "        \n",
    "              \n",
    "           txst = np.matrix([self.tx, self.txdot, self.txddot])\n",
    "           txnext = statetransition(txst, a[0])\n",
    "           self.tx = txnext[0,0]\n",
    "           self.txdot = txnext[0,1]%2.5\n",
    "           self.txddot = txnext[0,2]\n",
    "               \n",
    "           tyst = np.matrix([self.ty, self.tydot, self.tyddot])\n",
    "           tynext = statetransition(tyst, a[1])\n",
    "           self.ty = tynext[0,0]\n",
    "           self.tydot = tynext[0,1]%2.5\n",
    "           self.tyddot = tynext[0,2]\n",
    "               \n",
    "        # Reduce episode length by 1 second\n",
    "        self.episode_length -= 1       \n",
    "        \n",
    "        # Calculate reward\n",
    "        d1 = math.pow(math.pow((self.x1 - self.tx),2) + math.pow((self.y1 - self.ty),2) + math.pow((self.z1 - self.tz),2),0.5)\n",
    "        elevation1 = math.asin((self.z1 - self.tz)/d1)\n",
    "        lamda1 = 1/(1+alpha*math.pow(e,-beta*(elevation1-alpha)))\n",
    "        \n",
    "        d2 = math.pow(math.pow((x2 - self.tx),2) + math.pow((y2 - self.ty),2) + math.pow((z2 - self.tz),2),0.5)\n",
    "        elevation2 = math.asin((z2 - self.tz)/d2)\n",
    "        lamda2 = 1/(1+alpha*math.pow(e,-beta*(elevation2-alpha)))\n",
    "        \n",
    "        d3 = math.pow(math.pow((x3 - self.tx),2) + math.pow((y3 - self.ty),2) + math.pow((z3 - self.tz),2),0.5)\n",
    "        elevation3 = math.asin((z3-self.tz)/d3)\n",
    "        lamda3 = 1/(1+alpha*math.pow(e,-beta*(elevation3-alpha)))\n",
    "\n",
    "        \n",
    "        reward = 20*(lamda1 + lamda2 + lamda3 - lamdaprev) + 10*(lamda1 + lamda2 + lamda3) + 10*(lamda2 + lamda3)\n",
    "        lamda = lamda1 + lamda2 + lamda3\n",
    "               \n",
    "        # Check if episode is done\n",
    "        if self.episode_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "              \n",
    "        info = {}\n",
    "        \n",
    "        state = np.zeros(12)\n",
    "        state[0] = self.x1\n",
    "        state[1] = self.y1\n",
    "        state[2] = self.z1\n",
    "        state[3] = x2 - self.x1 \n",
    "        state[4] = y2 - self.y1\n",
    "        state[5] = z2 - self.z1\n",
    "        state[6] = x3 - self.x1\n",
    "        state[7] = y3 - self.y1\n",
    "        state[8] = z3 - self.z1\n",
    "        state[9] = self.tx - self.x1\n",
    "        state[10] = self.ty - self.y1\n",
    "        state[11] = self.tz - self.z1\n",
    "        \n",
    "        # Return step information\n",
    "        return state, reward, lamda, elevation1, done, info\n",
    "\n",
    "   \n",
    "    def reset(self,x,y,z,x2,y2,z2,x3,y3,z3):\n",
    "        # Reset states\n",
    "        self.x1 = x\n",
    "        self.y1 = y\n",
    "        self.z1 = z\n",
    "        \n",
    "        self.tx = 30\n",
    "        self.ty = 30\n",
    "        self.tz = 0\n",
    "        self.txdot = 0.4\n",
    "        self.tydot = 0.4\n",
    "        self.tzdot = 0\n",
    "        self.txddot = 0\n",
    "        self.tyddot = 0\n",
    "        self.tzddot = 0\n",
    "        \n",
    "        # Reset episode time\n",
    "        self.episode_length = Episode_length \n",
    "        done = False\n",
    "        \n",
    "        state = np.zeros(12)\n",
    "        state[0] = self.x1\n",
    "        state[1] = self.y1\n",
    "        state[2] = self.z1\n",
    "        state[3] = x2 - self.x1 \n",
    "        state[4] = y2 - self.y1\n",
    "        state[5] = z2 - self.z1\n",
    "        state[6] = x3 - self.x1\n",
    "        state[7] = y3 - self.y1\n",
    "        state[8] = z3 - self.z1\n",
    "        state[9] = self.tx - self.x1\n",
    "        state[10] = self.ty - self.y1\n",
    "        state[11] = self.tz - self.z1\n",
    "\n",
    "        return state\n",
    "    \n",
    "class DDQN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        self.d3 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        self.v = tf.keras.layers.Dense(1, activation=None)\n",
    "        self.a = tf.keras.layers.Dense(4, activation=None)\n",
    "\n",
    "    def call(self, input_data):\n",
    "        x = self.d1(input_data)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        v = self.v(x)\n",
    "        a = self.a(x)\n",
    "        Q = v +(a -tf.math.reduce_mean(a, axis=1, keepdims=True))\n",
    "        return Q\n",
    "\n",
    "    def advantage(self, state):\n",
    "        x = self.d1(state)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        a = self.a(x)\n",
    "        return a\n",
    "    \n",
    "class exp_replay():\n",
    "    def __init__(self, buffer_size= 5000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_mem = np.zeros((self.buffer_size, 12), dtype=np.float32)\n",
    "        self.action_mem = np.zeros((self.buffer_size), dtype=np.int32)\n",
    "        self.reward_mem = np.zeros((self.buffer_size), dtype=np.float32)\n",
    "        self.next_state_mem = np.zeros((self.buffer_size, 12), dtype=np.float32)\n",
    "        self.done_mem = np.zeros((self.buffer_size), dtype=np.bool)\n",
    "        self.pointer = 0\n",
    "\n",
    "    def add_exp(self, state, action, reward, next_state, done):\n",
    "        idx  = self.pointer % self.buffer_size \n",
    "        self.state_mem[idx] = state\n",
    "        self.action_mem[idx] = action\n",
    "        self.reward_mem[idx] = reward\n",
    "        self.next_state_mem[idx] = next_state\n",
    "        self.done_mem[idx] = 1 - int(done)\n",
    "        self.pointer += 1\n",
    "\n",
    "    def sample_exp(self, batch_size= 128):\n",
    "        max_mem = min(self.pointer, self.buffer_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        states = self.state_mem[batch]\n",
    "        actions = self.action_mem[batch]\n",
    "        rewards = self.reward_mem[batch]\n",
    "        next_states = self.next_state_mem[batch]\n",
    "        dones = self.done_mem[batch]\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "class agent():\n",
    "        def __init__(self, eps = 1.0, gamma=0.99, replace=100, lr=0.001):\n",
    "            self.gamma = gamma\n",
    "            self.epsilon = eps\n",
    "            self.min_epsilon = 0.001\n",
    "            self.epsilon_decay = 5e-4\n",
    "            self.replace = replace\n",
    "            self.trainstep = 0\n",
    "            self.memory = exp_replay()\n",
    "            self.batch_size = 128\n",
    "            self.q_net = DDQN()\n",
    "            self.target_net = DDQN()\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            self.q_net.compile(loss='mse', optimizer=opt)\n",
    "            self.target_net.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "\n",
    "        def act(self, state, test = 0):\n",
    "            if test ==1:\n",
    "                actions = self.q_net.advantage(np.array([state]))\n",
    "                action = np.argmax(actions)\n",
    "                return action\n",
    "            else:\n",
    "                if np.random.rand() <= self.epsilon:\n",
    "                   return np.random.choice([i for i in range(0,4)])\n",
    "\n",
    "                else:\n",
    "                   actions = self.q_net.advantage(np.array([state]))\n",
    "                   action = np.argmax(actions)\n",
    "                   return action\n",
    "\n",
    "\n",
    "        def update_mem(self, state, action, reward, next_state, done):\n",
    "            self.memory.add_exp(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "        def update_target(self):\n",
    "            self.target_net.set_weights(self.q_net.get_weights())     \n",
    "\n",
    "            \n",
    "        def update_epsilon(self):\n",
    "            self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.min_epsilon else self.min_epsilon\n",
    "            return self.epsilon\n",
    "\n",
    "          \n",
    "        def train(self):\n",
    "            if self.memory.pointer < self.batch_size:\n",
    "                return \n",
    "          \n",
    "            if self.trainstep % self.replace == 0:\n",
    "                self.update_target()\n",
    "            states, actions, rewards, next_states, dones = self.memory.sample_exp(self.batch_size)\n",
    "            target = self.q_net.predict(states)\n",
    "            next_state_val = self.target_net.predict(next_states)\n",
    "            max_action = np.argmax(self.q_net.predict(next_states), axis=1)\n",
    "            batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "            q_target = np.copy(target)\n",
    "            q_target[batch_index, actions] = rewards + self.gamma * next_state_val[batch_index, max_action]*dones\n",
    "            self.q_net.train_on_batch(states, q_target)\n",
    "            self.update_epsilon()\n",
    "            self.trainstep += 1\n",
    "\n",
    "#Code block to train the UAVs for 10000 episodes\n",
    "uav1 = UAV(20,25,25)\n",
    "uav2 = UAV(40,25,25)\n",
    "uav3 = UAV(40,40,25)\n",
    "\n",
    "K=Episode_length\n",
    "steps = 1000\n",
    "agents = agent()\n",
    "rewardplot = np.zeros(1000)\n",
    "for s in range(0,steps):\n",
    "    done = False\n",
    "    state1 = uav1.reset(20,25,25,40,25,25,40,40,25)\n",
    "    state2 = uav2.reset(40,25,25,40,40,25,20,25,25)\n",
    "    state3 = uav3.reset(40,40,25,20,25,25,40,25,25)\n",
    "    total_reward = 0\n",
    "    lamdaprev1 = 0\n",
    "    lamdaprev2 = 0\n",
    "    lamdaprev3 = 0\n",
    "    for i in range(0,K):\n",
    "          action1 = agents.act(state1)\n",
    "          action2 = agents.act(state2)\n",
    "          action3 = agents.act(state3)\n",
    "\n",
    "          next_state1, reward1, lamdaprev1, el, done, _ = uav1.step(state2[0], state2[1],state2[2],state3[0],state3[1],\\\n",
    "                                                                    state3[2], action1, lamdaprev3,i)\n",
    "          next_state2, reward2, lamdaprev2, el, done, _ = uav2.step(state3[0], state3[1],state3[2],state1[0],state1[1],\\\n",
    "                                                                    state1[2], action2, lamdaprev3,-1)\n",
    "          next_state3, reward3, lamdaprev3, el, done, _ = uav3.step(state1[0], state1[1],state1[2],state2[0],state2[1],\\\n",
    "                                                                    state2[2], action3, lamdaprev3,-1)\n",
    "          agents.update_mem(state1, action1, reward1, next_state1, done)\n",
    "          agents.update_mem(state2, action2, reward2, next_state2, done)\n",
    "          agents.update_mem(state3, action3, reward3, next_state3, done)\n",
    "          agents.train()\n",
    "          \n",
    "          state1 = next_state1\n",
    "          state2 = next_state2\n",
    "          state3 = next_state3\n",
    "          total_reward += (reward1 + reward2 + reward3)/3\n",
    "          \n",
    "    rewardplot[s] = total_reward\n",
    "    if done:\n",
    "       print(\"total reward after {} episode is {} and epsilon is {}\".format(s, total_reward, agents.epsilon))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7dd551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code block to test the agent\n",
    "uavtest1 = UAV(20,25,25)\n",
    "uavtest2 = UAV(40,25,25)\n",
    "uavtest3 = UAV(40,40,25)\n",
    "lamdaprev1 = 0\n",
    "lamdaprev2 = 0\n",
    "lamdaprev3 = 0\n",
    "\n",
    "lamdatotal = np.zeros(K)\n",
    "    \n",
    "\n",
    "K=Episode_length\n",
    "\n",
    "agenttest1 = agent()\n",
    "\n",
    "agenttest1.q_net = tf.keras.models.load_model('DRL_multi_UAV_q_net_lamda_10000')\n",
    "\n",
    "xuav1 = np.zeros(K)\n",
    "yuav1 = np.zeros(K)\n",
    "zuav1 = np.zeros(K)\n",
    "xuav2 = np.zeros(K)\n",
    "yuav2 = np.zeros(K)\n",
    "zuav2 = np.zeros(K)\n",
    "xuav3 = np.zeros(K)\n",
    "yuav3 = np.zeros(K)\n",
    "zuav3 = np.zeros(K)\n",
    "t_x = np.zeros(K)\n",
    "t_y = np.zeros(K)\n",
    "t_z = np.zeros(K)\n",
    "\n",
    "elplot1 = np.zeros(K)\n",
    "elplot2 = np.zeros(K)\n",
    "elplot3 = np.zeros(K)\n",
    "lamda1 = np.zeros(K)\n",
    "lamda2 = np.zeros(K)\n",
    "lamda3 = np.zeros(K)\n",
    "\n",
    "\n",
    "done = False\n",
    "state1 = uavtest1.reset(20,25,25,40,25,25,40,40,25)\n",
    "state2 = uavtest2.reset(40,25,25,40,40,25,20,25,25)\n",
    "state3 = uavtest3.reset(40,40,25,20,25,25,40,25,25)\n",
    "total_reward = 0\n",
    "for i in range(0,K):\n",
    "      xuav1[i] = state1[0]\n",
    "      yuav1[i] = state1[1]\n",
    "      zuav1[i] = state1[2]\n",
    "      xuav2[i] = state2[0]\n",
    "      yuav2[i] = state2[1]\n",
    "      zuav2[i] = state2[2]\n",
    "      xuav3[i] = state3[0]\n",
    "      yuav3[i] = state3[1]\n",
    "      zuav3[i] = state3[2]\n",
    "\n",
    "      t_x[i] = state1[9] + state1[0] \n",
    "      t_y[i] = state1[10] + state1[1]\n",
    "      t_z[i] = state1[11] + state1[2]\n",
    "      #i = i+1\n",
    "      x = agenttest1.q_net.d1(np.array([state1]))\n",
    "      x = agenttest1.q_net.d2(x)\n",
    "      x = agenttest1.q_net.d3(x)\n",
    "      actions1 = agenttest1.q_net.a(x)\n",
    "      action1 = np.argmax(actions1)\n",
    "      \n",
    "      x = agenttest1.q_net.d1(np.array([state2]))\n",
    "      x = agenttest1.q_net.d2(x)\n",
    "      x = agenttest1.q_net.d3(x)\n",
    "      actions2 = agenttest1.q_net.a(x)\n",
    "      action2 = np.argmax(actions2)\n",
    "      \n",
    "      x = agenttest1.q_net.d1(np.array([state3]))\n",
    "      x = agenttest1.q_net.d2(x)\n",
    "      x = agenttest1.q_net.d3(x)\n",
    "      actions3 = agenttest1.q_net.a(x)\n",
    "      action3 = np.argmax(actions3)\n",
    "            \n",
    "      next_state1 = state1\n",
    "      next_state2 = state2\n",
    "      next_state3 = state3\n",
    "    \n",
    "      next_state1, reward1, lamdaprev1, elplot1[i], done, _ = uavtest1.step(next_state2[0], next_state2[1],next_state2[2],next_state3[0], next_state3[1],next_state3[2], action1, lamdaprev3,i)\n",
    "      next_state2, reward2, lamdaprev2, elplot2[i], done, _ = uavtest2.step(next_state3[0], next_state3[1],next_state3[2],next_state1[0], next_state1[1],next_state1[2], action2, lamdaprev3,-1)\n",
    "      next_state3, reward3, lamdaprev3, elplot3[i], done, _ = uavtest3.step(next_state1[0], next_state1[1],next_state1[2],next_state2[0], next_state2[1],next_state2[2], action3, lamdaprev3,-1)\n",
    "    \n",
    "      state1 = next_state1\n",
    "      state2 = next_state2\n",
    "      state3 = next_state3\n",
    "      total_reward += (reward1 + reward2 + reward3)/3\n",
    "      \n",
    "    \n",
    "if done:\n",
    "   print(\"total reward after {} episode is {} and epsilon is {}\".format(1, total_reward, agenttest1.epsilon))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_tf] *",
   "language": "python",
   "name": "conda-env-ML_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
