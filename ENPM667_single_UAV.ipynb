{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db39fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "#implementation for single UAV case with one FR\n",
    "\n",
    "e = 2.718\n",
    "\n",
    "Episode_length = 20\n",
    "\n",
    "#map parameters based on obstacles\n",
    "alpha = 0.7               \n",
    "beta = 10\n",
    "        \n",
    "#defining the action space\n",
    "del_theta = 90*3.14/180    \n",
    "d = 5\n",
    "    \n",
    "#measurement noise parameters\n",
    "muLOS = 0\n",
    "stdLOS = 0.5\n",
    "muNLOS = 5\n",
    "stdNLOS = 5\n",
    "\n",
    "def gaussianpdf(x, mu, std):\n",
    "    pdfx = (math.pow(e,(-0.5*math.pow((x-mu)/std,2))))/(std*math.pow(2*3.14,0.5))\n",
    "    return pdfx  \n",
    "\n",
    "#maximum likelihood function for state estimotor(Not called in this code)\n",
    "def maxlf(uavx, uavy, uavz, tx, ty, tz):\n",
    "\n",
    "    d = math.pow(math.pow((uavx-tx),2) + math.pow((uavy-ty),2) + math.pow((uavz-tz),2),0.5)\n",
    "    elevation = math.asin((uavz-tz)/d)\n",
    "    lamda = 1/(1+alpha*math.pow(e,-beta*(elevation-alpha)))\n",
    "    \n",
    "    for i in range(tx-uavx-10,tx-uavx+10):\n",
    "        x=i\n",
    "        yij = lamda*gaussianpdf(x,tx-uavx+muLOS,stdLOS) + (1-lamda)*gaussianpdf(x,tx-uavx+muNLOS,stdNLOS)\n",
    "        plt.scatter(i,yij)\n",
    "    return yij\n",
    "\n",
    "#function to generate the next states of the target based on action 'a'\n",
    "def statetransition(stateold,a):\n",
    "    delK = 1\n",
    "    alpha0 = 0.95\n",
    "    \n",
    "    zerom = np.matrix([[0, 0, 0],[0, 0, 0], [0, 0, 0]])\n",
    "    phibar = np.matrix([[1, delK, delK*delK/2],[0, 1, delK], [0, 0, alpha0]])\n",
    "    Lbarv = np.matrix([delK*delK/2, delK, 0])\n",
    "    \n",
    "    statenew = np.dot(phibar,stateold.T).T + np.dot(a,Lbarv).T\n",
    "            \n",
    "    return statenew\n",
    "\n",
    "#Prediction of target state(Not called in this code)\n",
    "def target_pred(xvectorold, yvectorold, zvectorold, l):\n",
    "    \n",
    "    target_action = np.zeros((5,3))\n",
    "    target_action[0] = np.matrix([0, 0, 0])\n",
    "    target_action[1] = np.matrix([1, 0, 0])\n",
    "    target_action[2] = np.matrix([-1, 0, 0])\n",
    "    target_action[3] = np.matrix([0, 1, 0])\n",
    "    target_action[4] = np.matrix([0, -1, 0])\n",
    "\n",
    "    lbar = np.zeros(5)\n",
    "    lbar[0] = 0\n",
    "    lbar[1] = 1\n",
    "    lbar[2] = 2\n",
    "    lbar[3] = 3\n",
    "    lbar[4] = 4\n",
    "    \n",
    "    for j in range(0,5):\n",
    "        if lbar[j] == l:\n",
    "            transprob = 0.1\n",
    "        else:\n",
    "            transprob = 0.225\n",
    "        xvectornew = statetransition(xvectorold, target_action[int(lbar[j])][0]).T\n",
    "        yvectornew = statetransition(yvectorold, target_action[int(lbar[j])][1]).T\n",
    "        zvectornew = statetransition(zvectorold, target_action[int(lbar[j])][2]).T\n",
    "           \n",
    "    return xvectornew, yvectornew, zvectornew, state_prob\n",
    "\n",
    "class UAV(Env):\n",
    "    def __init__(self):\n",
    "        # Actions UAV can take\n",
    "        self.uav_action_space = Discrete(4)\n",
    "        self.x1 = 20\n",
    "        self.y1 = 25\n",
    "        self.z1 = 25\n",
    "        \n",
    "        self.tx = 30\n",
    "        self.ty = 30\n",
    "        self.tz = 0\n",
    "        self.txdot = 0.4\n",
    "        self.tydot = 0.4\n",
    "        self.tzdot = 0\n",
    "        self.txddot = 0\n",
    "        self.tyddot = 0\n",
    "        self.tzddot = 0\n",
    "        \n",
    "        \n",
    "       # Set episode length\n",
    "        self.episode_length = Episode_length\n",
    "        \n",
    "    def step(self, action, lamdaprev,i):\n",
    "        # Apply UAV action\n",
    "        self.x1 += d*math.cos(action*del_theta)\n",
    "        self.y1 += d*math.sin(action*del_theta)\n",
    "        self.z1 += 0\n",
    "        \n",
    "        target_action = np.zeros((5,3))\n",
    "        target_action[0] = np.matrix([0, 0, 0])\n",
    "        target_action[1] = np.matrix([1, 0, 0])\n",
    "        target_action[2] = np.matrix([-1, 0, 0])\n",
    "        target_action[3] = np.matrix([0, 1, 0])\n",
    "        target_action[4] = np.matrix([0, -1, 0])\n",
    "       \n",
    "    \n",
    "        a = target_action[random.randint(0,4)]\n",
    "        \n",
    "              \n",
    "        txst = np.matrix([self.tx, self.txdot, self.txddot])\n",
    "        txnext = statetransition(txst, a[0])\n",
    "        self.tx = txnext[0,0]\n",
    "        self.txdot = txnext[0,1]%2.5\n",
    "        self.txddot = txnext[0,2]\n",
    "               \n",
    "        tyst = np.matrix([self.ty, self.tydot, self.tyddot])\n",
    "        tynext = statetransition(tyst, a[1])\n",
    "        self.ty = tynext[0,0]\n",
    "        self.tydot = tynext[0,1]%2.5\n",
    "        self.tyddot = tynext[0,2]\n",
    "               \n",
    "        # Reduce episode length by 1 second\n",
    "        self.episode_length -= 1       \n",
    "        \n",
    "        # Calculate reward\n",
    "        d1 = math.pow(math.pow((self.x1 - self.tx),2) + math.pow((self.y1 - self.ty),2) + math.pow((self.z1 - self.tz),2),0.5)\n",
    "        elevation1 = math.asin((self.z1 - self.tz)/d1)\n",
    "        lamda1 = 1/(1+alpha*math.pow(e,-beta*(elevation1-alpha)))\n",
    "        \n",
    "        reward = 20*(lamda1-lamdaprev)\n",
    "        lamda = lamda1\n",
    "        \n",
    "         \n",
    "        # Check if episode is done\n",
    "        if self.episode_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "              \n",
    "        info = {}\n",
    "        \n",
    "        state = np.zeros(6)\n",
    "        state[0] = self.x1\n",
    "        state[1] = self.y1\n",
    "        state[2] = self.z1\n",
    "        state[3] = self.tx - self.x1\n",
    "        state[4] = self.ty - self.y1\n",
    "        state[5] = self.tz - self.z1\n",
    "       \n",
    "        # Return step information\n",
    "        return state, reward, lamda, elevation1,done, info\n",
    "\n",
    "   \n",
    "    def reset(self):\n",
    "        # Reset states\n",
    "        self.x1 = 20\n",
    "        self.y1 = 25\n",
    "        self.z1 = 25\n",
    "        \n",
    "        self.tx = 30\n",
    "        self.ty = 30\n",
    "        self.tz = 0\n",
    "        self.txdot = 0.4\n",
    "        self.tydot = 0.4\n",
    "        self.tzdot = 0\n",
    "        self.txddot = 0\n",
    "        self.tyddot = 0\n",
    "        self.tzddot = 0\n",
    "        \n",
    "        # Reset episode time and states\n",
    "        self.episode_length = Episode_length \n",
    "        done = False\n",
    "        \n",
    "        state = np.zeros(6)\n",
    "        state[0] = self.x1\n",
    "        state[1] = self.y1\n",
    "        state[2] = self.z1\n",
    "        state[3] = self.tx - self.x1\n",
    "        state[4] = self.ty - self.y1\n",
    "        state[5] = self.tz - self.z1\n",
    "\n",
    "        return state\n",
    "\n",
    "#Duel Deep Q-learning architecture\n",
    "class DDQN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        self.d3 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        self.v = tf.keras.layers.Dense(1, activation=None)\n",
    "        self.a = tf.keras.layers.Dense(4, activation=None)\n",
    "\n",
    "    def call(self, input_data):\n",
    "        x = self.d1(input_data)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        v = self.v(x)\n",
    "        a = self.a(x)\n",
    "        Q = v +(a -tf.math.reduce_mean(a, axis=1, keepdims=True))\n",
    "        return Q\n",
    "\n",
    "    def advantage(self, state):\n",
    "        x = self.d1(state)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        a = self.a(x)\n",
    "        return a\n",
    "\n",
    "#Used to fill and sample tuples for training\n",
    "class exp_replay():\n",
    "    def __init__(self, buffer_size= 5000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_mem = np.zeros((self.buffer_size, 6), dtype=np.float32)\n",
    "        self.action_mem = np.zeros((self.buffer_size), dtype=np.int32)\n",
    "        self.reward_mem = np.zeros((self.buffer_size), dtype=np.float32)\n",
    "        self.next_state_mem = np.zeros((self.buffer_size, 6), dtype=np.float32)\n",
    "        self.done_mem = np.zeros((self.buffer_size), dtype=np.bool)\n",
    "        self.pointer = 0\n",
    "\n",
    "    def add_exp(self, state, action, reward, next_state, done):\n",
    "        idx  = self.pointer % self.buffer_size \n",
    "        self.state_mem[idx] = state\n",
    "        self.action_mem[idx] = action\n",
    "        self.reward_mem[idx] = reward\n",
    "        self.next_state_mem[idx] = next_state\n",
    "        self.done_mem[idx] = 1 - int(done)\n",
    "        self.pointer += 1\n",
    "\n",
    "    def sample_exp(self, batch_size= 128):\n",
    "        max_mem = min(self.pointer, self.buffer_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        states = self.state_mem[batch]\n",
    "        actions = self.action_mem[batch]\n",
    "        rewards = self.reward_mem[batch]\n",
    "        next_states = self.next_state_mem[batch]\n",
    "        dones = self.done_mem[batch]\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "#Initializes and invokes the necessary functions for training and testing    \n",
    "class agent():\n",
    "        def __init__(self, eps = 1.0, gamma=0.99, replace=100, lr=0.001):\n",
    "            self.gamma = gamma\n",
    "            self.epsilon = eps\n",
    "            self.min_epsilon = 0.0001\n",
    "            self.epsilon_decay = 5e-5\n",
    "            self.replace = replace\n",
    "            self.trainstep = 0\n",
    "            self.memory = exp_replay()\n",
    "            self.batch_size = 128\n",
    "            self.q_net = DDQN()\n",
    "            self.target_net = DDQN()\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            self.q_net.compile(loss='mse', optimizer=opt)\n",
    "            self.target_net.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        #Implemetation of the epsilon-greedy strategy\n",
    "        def act(self, state, test = 0):\n",
    "            if test ==1:\n",
    "                actions = self.q_net.advantage(np.array([state]))\n",
    "                action = np.argmax(actions)\n",
    "                return action\n",
    "            else:\n",
    "                if np.random.rand() <= self.epsilon:\n",
    "                   return np.random.choice([i for i in range(0,4)])\n",
    "\n",
    "                else:\n",
    "                   actions = self.q_net.advantage(np.array([state]))\n",
    "                   action = np.argmax(actions)\n",
    "                   return action\n",
    "\n",
    "\n",
    "        def update_mem(self, state, action, reward, next_state, done):\n",
    "            self.memory.add_exp(state, action, reward, next_state, done)\n",
    "\n",
    "        #target function is updated every 100 iterations\n",
    "        def update_target(self):\n",
    "            self.target_net.set_weights(self.q_net.get_weights())     \n",
    "\n",
    "        def update_epsilon(self):\n",
    "            self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.min_epsilon else self.min_epsilon\n",
    "            return self.epsilon\n",
    "\n",
    "          \n",
    "        def train(self):\n",
    "            if self.memory.pointer < self.batch_size:\n",
    "                return \n",
    "          \n",
    "            if self.trainstep % self.replace == 0:\n",
    "                self.update_target()\n",
    "            states, actions, rewards, next_states, dones = self.memory.sample_exp(self.batch_size)\n",
    "            target = self.q_net.predict(states)\n",
    "            next_state_val = self.target_net.predict(next_states)\n",
    "            max_action = np.argmax(self.q_net.predict(next_states), axis=1)\n",
    "            batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "            q_target = np.copy(target)\n",
    "            q_target[batch_index, actions] = rewards + self.gamma * next_state_val[batch_index, max_action]*dones\n",
    "            self.q_net.train_on_batch(states, q_target)\n",
    "            self.update_epsilon()\n",
    "            self.trainstep += 1\n",
    "\n",
    "#Code block to train the agent for 10000 episodes\n",
    "uav = UAV()\n",
    "CRLBprev = 0\n",
    "\n",
    "K=Episode_length\n",
    "steps = 10000\n",
    "rewardplot = np.zeros(10000)\n",
    "agents = agent()\n",
    "for s in range(0,steps):\n",
    "    done = False\n",
    "    state = uav.reset()\n",
    "    total_reward = 0\n",
    "    CRLBprev = 0\n",
    "    for i in range(0,K):\n",
    "          action = agents.act(state)\n",
    "          next_state, reward, CRLBprev, done, _ = uav.step(action, CRLBprev,i)\n",
    "          agents.update_mem(state, action, reward, next_state, done)\n",
    "          agents.train()\n",
    "          \n",
    "          state = next_state\n",
    "          total_reward += reward\n",
    "    \n",
    "    rewardplot[s] = total_reward\n",
    "    if done:\n",
    "       print(\"total reward after {} episode is {} and epsilon is {}\".format(s, total_reward, agents.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b5299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code block to test the agent\n",
    "\n",
    "uavtest = UAV()\n",
    "lamdaprev = 0\n",
    "\n",
    "K=Episode_length\n",
    "\n",
    "agenttest1 = agent()\n",
    "\n",
    "#loading the trained model saved as \\tmp\\DRL_single_UAV_q_net_lamda_10000\n",
    "agenttest1.q_net = tf.keras.models.load_model('DRL_single_UAV_q_net_lamda_10000')\n",
    "                                                                                 \n",
    "xuav1 = np.zeros(K)\n",
    "yuav1 = np.zeros(K)\n",
    "zuav1 = np.zeros(K)\n",
    "t_x = np.zeros(K)\n",
    "t_y = np.zeros(K)\n",
    "t_z = np.zeros(K)\n",
    "lamdaplot = np.zeros(K)\n",
    "elplot = np.zeros(K)\n",
    "\n",
    "done = False\n",
    "state = uavtest.reset()\n",
    "total_reward = 0\n",
    "for i in range(0,K):\n",
    "      xuav1[i] = state[0]\n",
    "      yuav1[i] = state[1]\n",
    "      zuav1[i] = state[2]\n",
    "      t_x[i] = state[3] + state[0] \n",
    "      t_y[i] = state[4] + state[1]\n",
    "      t_z[i] = state[5] + state[2]\n",
    "\n",
    "      x = agenttest1.q_net.d1(np.array([state]))\n",
    "      x = agenttest1.q_net.d2(x)\n",
    "      x = agenttest1.q_net.d3(x)\n",
    "      actions = agenttest1.q_net.a(x)\n",
    "      action = np.argmax(actions)\n",
    "\n",
    "      next_state, reward, lamdaprev, elplot[i], done, _ = uavtest.step(action, lamdaprev,i)\n",
    "      state = next_state\n",
    "      total_reward += reward\n",
    "    \n",
    "      lamdaplot[i] = lamdaprev\n",
    "      \n",
    "    \n",
    "if done:\n",
    "   print(\"total reward after {} episode is {} and epsilon is {}\".format(1, total_reward, agenttest1.epsilon))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_tf] *",
   "language": "python",
   "name": "conda-env-ML_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
